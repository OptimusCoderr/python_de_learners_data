{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-03T08:24:46.192326Z","iopub.execute_input":"2023-04-03T08:24:46.192882Z","iopub.status.idle":"2023-04-03T08:24:58.487094Z","shell.execute_reply.started":"2023-04-03T08:24:46.192825Z","shell.execute_reply":"2023-04-03T08:24:58.485102Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspark in /opt/conda/lib/python3.7/site-packages (3.3.2)\nRequirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark) (0.10.9.5)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Ways of Clusters : Spark Part 2\n\n\nIn part1 that is hosted https://www.kaggle.com/code/kamaljp/5ways-spark-waysof-partitioning discusses the way to divide and cluster the data. This notebook discusses how to read them.","metadata":{}},{"cell_type":"code","source":"#Creating the Spark Session\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\nspark = SparkSession. \\\n    builder. \\\n    appName('Read_Data'). \\\n    getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:24:58.489765Z","iopub.execute_input":"2023-04-03T08:24:58.490195Z","iopub.status.idle":"2023-04-03T08:25:04.977501Z","shell.execute_reply.started":"2023-04-03T08:24:58.490152Z","shell.execute_reply":"2023-04-03T08:25:04.976346Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"23/04/03 08:25:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Before we can Read\n\nWe have to find a dataset that can be used for reading into Spark Tables. Instead for searching for the new dataset, we are going to create a couple. Please follow part one for your reference","metadata":{}},{"cell_type":"code","source":"#importing the dataset\n\nsales_raw = spark.read.csv('/kaggle/input/datasetbackups/dmart/Sales.csv',\n                          inferSchema=True, sep='\\t',header=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:26:19.559800Z","iopub.execute_input":"2023-04-03T08:26:19.561209Z","iopub.status.idle":"2023-04-03T08:26:30.273822Z","shell.execute_reply.started":"2023-04-03T08:26:19.561127Z","shell.execute_reply":"2023-04-03T08:26:30.272121Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"#Creating database so the Spark SQL tables can be created. \nspark.sql(\"CREATE DATABASE dmart_db\")\nspark.sql(\"USE dmart_db\")","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:26:30.276678Z","iopub.execute_input":"2023-04-03T08:26:30.278205Z","iopub.status.idle":"2023-04-03T08:26:30.453982Z","shell.execute_reply.started":"2023-04-03T08:26:30.278122Z","shell.execute_reply":"2023-04-03T08:26:30.452904Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DataFrame[]"},"metadata":{}}]},{"cell_type":"code","source":"SQL = spark.sql\nSQL(\"SET spark.sql.shuffle.partitions = 2;\")","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:26:45.668499Z","iopub.execute_input":"2023-04-03T08:26:45.669166Z","iopub.status.idle":"2023-04-03T08:26:45.715473Z","shell.execute_reply.started":"2023-04-03T08:26:45.669123Z","shell.execute_reply":"2023-04-03T08:26:45.714391Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DataFrame[key: string, value: string]"},"metadata":{}}]},{"cell_type":"code","source":"sales_raw.printSchema()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T03:46:19.669466Z","iopub.execute_input":"2023-04-03T03:46:19.670028Z","iopub.status.idle":"2023-04-03T03:46:19.679357Z","shell.execute_reply.started":"2023-04-03T03:46:19.669985Z","shell.execute_reply":"2023-04-03T03:46:19.677596Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets create a table that has partition builtin\n\nsales_raw.write.saveAsTable(\"sales_partition\",\n                           mode='overwrite',\n                           partitionBy='GMV',\n                           format='parquet')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:26:53.751074Z","iopub.execute_input":"2023-04-03T08:26:53.751680Z","iopub.status.idle":"2023-04-03T08:31:02.965633Z","shell.execute_reply.started":"2023-04-03T08:26:53.751633Z","shell.execute_reply":"2023-04-03T08:31:02.964187Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"## We have the first one completed. Starting the second data\n\nThis will contain partitions based on two columns","metadata":{}},{"cell_type":"code","source":"#Lets take another file\n\nfirst_sales = spark.read.csv(\"/kaggle/input/datasetbackups/dmart/firstfile.csv\",\n                            inferSchema=True,header=True,sep=',')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:31:02.969039Z","iopub.execute_input":"2023-04-03T08:31:02.969544Z","iopub.status.idle":"2023-04-03T08:31:07.064900Z","shell.execute_reply.started":"2023-04-03T08:31:02.969501Z","shell.execute_reply":"2023-04-03T08:31:07.063539Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"trimed_sales = first_sales.selectExpr(\"split_part(Date, ' ',1) as day_date\",\n                    \"Sales_name\", \"gmv_new\", \"units\")","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:31:07.346989Z","iopub.execute_input":"2023-04-03T08:31:07.347829Z","iopub.status.idle":"2023-04-03T08:31:07.440465Z","shell.execute_reply.started":"2023-04-03T08:31:07.347756Z","shell.execute_reply":"2023-04-03T08:31:07.438666Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"trimed_sales.write.saveAsTable(\"trimed_sales_partition\",\n                              mode='overwrite',\n                              format='parquet',\n                              partitionBy=['Sales_name','gmv_new'])","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:31:07.442182Z","iopub.execute_input":"2023-04-03T08:31:07.445354Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"[Stage 9:======>                                               (262 + 4) / 2218]\r","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/working/spark-warehouse/dmart_db.db/sales_partition/ | head -n 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SQL(\"SHOW PARTITIONS trimed_sales_partition\").show(2,truncate=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/spark-warehouse/dmart_db.db/trimed_sales_partition/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/spark-warehouse/dmart_db.db/trimed_sales_partition/Sales_name\\=BED | head -n 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The third one... Really!!!","metadata":{}},{"cell_type":"code","source":"trimed_sales.write.bucketBy(10, 'Sales_name').saveAsTable(\"bucketed_sales\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/spark-warehouse/dmart_db.db/bucketed_sales/ ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now we have the files. \n\nLets talk about loading these files into the Spark Cluster as Tables\n\n******* Example Code *********\n\nCREATE TABLE student (id INT, \n                        name STRING, \n                         age INT)\n\nUSING CSV\n\nPARTITIONED BY (age)\n\nCLUSTERED BY (Id) INTO 4 buckets;","metadata":{}},{"cell_type":"code","source":"#We will be using the spark read api for doing the same","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}