# -*- coding: utf-8 -*-
"""exploring_gpt4all

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MQSpApD-yzV7kMo_Wv84bmjdI04pU3f3
"""

from huggingface_hub import hf_hub_download

#Download the model
#hf_hub_download(repo_id="LLukas22/gpt4all-lora-quantized-ggjt", 
#                filename="ggjt-model.bin", 
#                local_dir=".")

from langchain.llms import LlamaCpp
from langchain import PromptTemplate, LLMChain

template = """Question: {question}

Answer: Let's think step by step."""

prompt = PromptTemplate(template=template, input_variables=["question"])

llm = LlamaCpp(model_path="ggjt-model.bin")

question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"

print(f"""Querying the llamaCpp llm directly. Without any support function.\n
      The question is :\n
      {question}""")

llm(question)

llm_chain = LLMChain(prompt=prompt, llm=llm)

llm_chain.run(question)

print("Working out the Pandas Dataframe Agent with LLamaCpp...\n")
#Two areas to check.
from langchain.chains import LLMRequestsChain
from langchain.agents import create_pandas_dataframe_agent

import pandas as pd

df = pd.read_csv('space_shortened.csv')

gpt4llagent = create_pandas_dataframe_agent(llm,
                                      df, 
                                      verbose=True)
print("Agent setting done. Now querying it... ")

try:
    gpt4llagent.run("How many travellers are in the dataset?")

except Exception as e:

    print(e)

print("Starting to setup LLM Requests Chain...")

req_template = """Between >>> and <<< are the raw search result text from google.
Extract the answer to the question '{query}' or say "not found" if the information is not contained.
Use the format
Extracted:<answer or "not found">
>>> {requests_result} <<<
Extracted:"""

req_prompt = PromptTemplate(
    input_variables=["query", "requests_result"],
    template=req_template,
)

netchain = LLMRequestsChain(llm_chain = LLMChain(llm=llm, 
                                              prompt=req_prompt))

question = "What are the Three biggest states in India and its population?"

inputs = {
    "query": question,
    "url": "https://www.google.com/search?q=" + question.replace(" ", "+")
}

print("Done setting up requests chain. Executing it")
try:
    netchain(inputs)

except Exception as e:
    print(e)

from langchain.agents import load_tools
from langchain.agents import initialize_agent

import os
import sys
os.environ['SERPER_API_KEY']= sys.argv[1]

tools = load_tools(["google-serper"], llm=llm)

agent = initialize_agent(tools, 
                         llm, 
                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, 
                         verbose=True)

print("Executing the agent with tools...")
try:
    agent.run("What is the weather in Delhi?")

except Exception as e:

    print(e)





from langchain.embeddings import LlamaCppEmbeddings

llama = LlamaCppEmbeddings(model_path="/content/ggjt-model.bin")

text = "This is a test document."

llama.embed_query(text)

